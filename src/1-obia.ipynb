{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Object Based Image Analysis (OBIA)\n",
    "\n",
    "1. Import and preprocess images (remove outliers and rescale intensity)\n",
    "2. Segment images (parameters per stratum)\n",
    "3. Vectorize segments, then extract features for each segment\n",
    "4. Train classification model and output results (and scaler)\n",
    "\n",
    "Now parallelized :sunglasses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "from functools import partial\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import rasterio\n",
    "\n",
    "from joblib import dump\n",
    "from rasterio.features import shapes\n",
    "from sklearn import svm\n",
    "from sklearn import preprocessing\n",
    "from skimage.exposure import rescale_intensity\n",
    "from skimage.segmentation import felzenszwalb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import VarianceThreshold, SelectKBest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report, cohen_kappa_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from fetex_functions import get_features_cols, extract_feature_df\n",
    "from segmentation_functions import get_fid, read_stratum_images, tiff_export, gpkg_export\n",
    "\n",
    "pd.options.display.max_columns = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of cores to use for multiprocessing\n",
    "nc = os.cpu_count()\n",
    "\n",
    "# Samples to ignore (for now)\n",
    "blacklist = [str(x) for x in [10, 17, 22]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rescale_band(band, old_min, old_max, new_min, new_max):\n",
    "    return ((band - old_min)/old_max) * (new_max - new_min)\n",
    "\n",
    "def reject_outliers(data, m, no_data):\n",
    "    data_mean = np.mean(data[data != no_data])\n",
    "    data_std = np.std(data[data != no_data])\n",
    "    max_val = data_mean + m * data_std\n",
    "    min_val = data_mean - m * data_std\n",
    "    \n",
    "    result = np.copy(data)\n",
    "    result[result > max_val] = result[result < max_val].max() # Assign next highest existent val\n",
    "    result[result < min_val] = result[result > min_val].min() # Same w/ lowest for min\n",
    "\n",
    "    return result\n",
    "\n",
    "def process_band(band, m=5, no_data=0, out_type='uint8', out_range=(0, 255), elev_band=False, elev_bounds=(0,25)):\n",
    "    min_elev, max_elev = elev_bounds\n",
    "    min_out, max_out = out_range\n",
    "    if elev_band:\n",
    "        # Trust min but not max\n",
    "        band[band < min_elev] = min_elev\n",
    "        band[band > max_elev] = band[(band >= min_elev) & (band <= max_elev)].mean()\n",
    "        \n",
    "        # Normalize to bounds (min_elev -> min_out, max_elev -> max_out)\n",
    "        band = rescale_band(band, min_elev, max_elev, min_out, max_out)\n",
    "        return band.astype(out_type)\n",
    "    else:\n",
    "        return rescale_intensity(\n",
    "            reject_outliers(band, m, no_data),\n",
    "            out_range=out_type\n",
    "        ).astype(out_type)\n",
    "    \n",
    "def rescale_clean_img(name, img, elev_band=4, debug=True):\n",
    "    # Handle super-weird data\n",
    "    if debug:\n",
    "        print(f'Working on {name}')\n",
    "    img = img['img']\n",
    "    img[img < 0 ] = 0\n",
    "\n",
    "    # Then just normal-weird\n",
    "    # We'll scale each band independently\n",
    "    # Last band is LiDAR so treatment is different\n",
    "    processed_bands = []\n",
    "    for band in range(0, img.shape[2]):\n",
    "        is_elev_band = True if band == elev_band else False\n",
    "        processed_bands.append(process_band(img[:,:,band], elev_band=is_elev_band))\n",
    "            \n",
    "    return (name, np.dstack(processed_bands))\n",
    "\n",
    "## Multiprocessing wrappers\n",
    "\n",
    "def read_imgs_mult(stratum):\n",
    "    return read_stratum_images(base_path='../data/imgs_final/', stratum=stratum, blacklist=blacklist)\n",
    "\n",
    "def felz_mult(name, img_8, scale, sigma, min_size, debug=False, bands=None):\n",
    "    if bands:\n",
    "        s = felzenszwalb(img_8[:,:,bands], scale=scale, sigma=sigma, min_size=min_size)\n",
    "    else:\n",
    "        s = felzenszwalb(img_8, scale=scale, sigma=sigma, min_size=min_size)\n",
    "    if debug:\n",
    "        print(f\"Img: {name}. Number of segments: {len(np.unique(s))}\")\n",
    "    \n",
    "    return name, s\n",
    "\n",
    "# Exports\n",
    "\n",
    "def tiff_export_mult(sample_name, sample_segments):\n",
    "    tiff_export(\n",
    "        segment=sample_segments,\n",
    "        props=imgs[sample_name]['props'],\n",
    "        output_name=os.path.join(segmentation_path, sample_name),\n",
    "        debug=True\n",
    "    )\n",
    "\n",
    "def gpkg_export_mult(sample_name, sample_segments):\n",
    "    gpkg_export(\n",
    "        segment=sample_segments,\n",
    "        props=imgs[sample_name]['props'],\n",
    "        output_name=os.path.join(segmentation_path, sample_name.replace('.tif', '.gpkg')),\n",
    "        debug=True\n",
    "    )\n",
    "\n",
    "# Feature extraction\n",
    "\n",
    "def fetex_multi(sample_name, debug=True):\n",
    "    if debug:\n",
    "        print(f'------ Working on {sample_name} ------')\n",
    "    img = imgs_8[sample_name]\n",
    "    seg = segments[sample_name]\n",
    "    cols = get_features_cols(img.shape[2])\n",
    "    \n",
    "    return sample_name, extract_feature_df(img, seg, cols, debug=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading and scaling to uint8 for all bands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "strata = {\n",
    "    'residential': (100, 1, 1000),\n",
    "    'urbanreg': (70, 1, 800),\n",
    "    'urbanirreg': (60, 1, 600),\n",
    "    'rural': (50, 1, 600),\n",
    "    'shanty':  (50, 1, 600)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished reading. Scaling...\n",
      "Working on sample_13_residential.tif\n",
      "Working on sample_6_residential.tif\n",
      "Working on sample_0_residential.tif\n",
      "Working on sample_23_residential.tif\n",
      "Working on sample_8_residential.tif\n",
      "Working on sample_11_urbanreg.tif\n",
      "Working on sample_20_urbanreg.tif\n",
      "Working on sample_4_urbanreg.tif\n",
      "Working on sample_1_urbanreg.tif\n",
      "Working on sample_7_urbanreg.tif\n",
      "Working on sample_14_urbanreg.tif\n",
      "Working on sample_12_urbanreg.tif\n",
      "Working on sample_21_urbanirreg.tif\n",
      "Working on sample_15_urbanirreg.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/skimage/exposure/exposure.py:351: RuntimeWarning: invalid value encountered in true_divide\n",
      "  image = (image - imin) / float(imax - imin)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on sample_18_urbanirreg.tif\n",
      "Working on sample_16_urbanirreg.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/skimage/exposure/exposure.py:351: RuntimeWarning: invalid value encountered in true_divide\n",
      "  image = (image - imin) / float(imax - imin)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on sample_5_rural.tif\n",
      "Working on sample_19_rural.tif\n",
      "Working on sample_3_shanty.tif\n",
      "Working on sample_9_shanty.tif\n",
      "Working on sample_2_shanty.tif\n"
     ]
    }
   ],
   "source": [
    "with Pool(nc) as pool:\n",
    "    imgs_list = pool.map(read_imgs_mult, strata.keys())\n",
    "    # Reduce array to dict\n",
    "    imgs = {k:v for d in imgs_list for k, v in d.items()}\n",
    "    print('Finished reading. Scaling...')\n",
    "    imgs_8 = {k:v for k,v in pool.starmap(rescale_clean_img, imgs.items())}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segmenting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll prepare an iterable for the `felz_mult` parallelized function containing the image name,\n",
    "the image itself and the segmentation parameters for its stratum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segm_iterable = [(k,v,*strata[k.split('_')[-1].replace('.tif', '')], True, [0,1,2,4]) for k,v in imgs_8.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Pool(nc) as pool:\n",
    "    segments = {k:v for k,v in pool.starmap(felz_mult, segm_iterable)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmentation_path = \"../data/imgs_segm/\"\n",
    "if not os.path.exists(segmentation_path):\n",
    "    os.mkdir(segmentation_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Pool(nc) as pool:\n",
    "    pool.starmap(tiff_export_mult, segments.items())\n",
    "    pool.starmap(gpkg_export_mult, segments.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment and run this cell to **load segmented images**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments_list = [read_stratum_images(x, segmentation_path) for x in strata.keys()]\n",
    "segments = {k: v['img'][:,:,0] for d in segments_list for k, v in d.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add synthetic bands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, img in imgs_8.items():\n",
    "    b7 = rescale_intensity(1000 + img[:,:,1] - img[:,:,0], out_range=np.uint8).astype(np.uint8)\n",
    "    b8 = rescale_intensity(1000 + img[:,:,2] - img[:,:,1], out_range=np.uint8).astype(np.uint8)\n",
    "    b9 = rescale_intensity(1000 + img[:,:,0] - img[:,:,2], out_range=np.uint8).astype(np.uint8)\n",
    "    imgs_8[name] = np.dstack((img, b7, b8, b9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reconvert height\n",
    "\n",
    "We resampled the height band to the full `np.uint8` interval `(0, 255)` for the segmentation but we need to now keep those values as actual meters. Later, in the classification, these values will actually be normalized again, but we need the originals for the MBT classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, img in imgs_8.items():\n",
    "    imgs_8[name][:,:,4] = rescale_band(imgs_8[name][:,:,4], 0, 255, 0, 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Working on sample_0_residential.tif ------\n",
      "------ Working on sample_13_residential.tif ------\n",
      "------ Working on sample_8_residential.tif ------\n",
      "------ Working on sample_20_urbanreg.tif ------\n",
      "630 segments to process\n",
      "758 segments to process\n",
      "643 segments to process\n",
      "739 segments to process\n",
      "------ Working on sample_11_urbanreg.tif ------\n",
      "685 segments to process\n",
      "------ Working on sample_4_urbanreg.tif ------\n",
      "604 segments to process\n",
      "------ Working on sample_23_residential.tif ------\n",
      "591 segments to process\n",
      "------ Working on sample_6_residential.tif ------\n",
      "550 segments to process\n",
      "------ Working on sample_1_urbanreg.tif ------\n",
      "637 segments to process\n",
      "------ Working on sample_14_urbanreg.tif ------\n",
      "657 segments to process\n",
      "------ Working on sample_21_urbanirreg.tif ------\n",
      "920 segments to process\n",
      "------ Working on sample_18_urbanirreg.tif ------\n",
      "1388 segments to process\n",
      "------ Working on sample_7_urbanreg.tif ------\n",
      "713 segments to process\n",
      "------ Working on sample_12_urbanreg.tif ------\n",
      "726 segments to process\n",
      "------ Working on sample_15_urbanirreg.tif ------\n",
      "1115 segments to process\n",
      "------ Working on sample_5_rural.tif ------\n",
      "932 segments to process\n",
      "------ Working on sample_3_shanty.tif ------\n",
      "837 segments to process\n",
      "------ Working on sample_2_shanty.tif ------\n",
      "999 segments to process\n",
      "------ Working on sample_16_urbanirreg.tif ------\n",
      "1003 segments to process\n",
      "------ Working on sample_9_shanty.tif ------\n",
      "931 segments to process\n",
      "------ Working on sample_19_rural.tif ------\n",
      "1088 segments to process\n"
     ]
    }
   ],
   "source": [
    "with Pool(nc) as pool:\n",
    "    fdfs = {k:v for k,v in pool.map(fetex_multi, segments.keys())}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make ID unique to each sample by prefixing the FID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample, df in fdfs.items():\n",
    "    df['id'] = df['id'].apply(lambda x: f'{get_fid(sample)}_{x}')\n",
    "\n",
    "fdf = pd.concat(fdfs).reset_index()\n",
    "fdf.drop(axis=1, columns=['level_0', 'level_1', 'index'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load GCPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcps_path = os.path.abspath('../data/gcps/')\n",
    "gcps = {os.path.basename(k).replace('.gpkg', ''):gpd.read_file(k) \n",
    "        for k in [os.path.join(gcps_path, x) \n",
    "                  for x in os.listdir(gcps_path) if x.endswith('gpkg')]}\n",
    "    \n",
    "gcps = pd.concat(gcps).reset_index()\n",
    "gcps.rename(index=str, columns={'level_0': 'class', 'level_1': 'idx'}, inplace=True)\n",
    "\n",
    "gcps.loc[gcps['class'].isin(['red_tin', 'blue_tin']), 'class'] = 'tin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class\n",
       "concrete      335\n",
       "pavement      226\n",
       "shadow        169\n",
       "tin           397\n",
       "vegetation    172\n",
       "Name: idx, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gcps.groupby('class')['idx'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this line only for LiDAR segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gcps = gcps.loc[gcps['class'] != 'shadow'].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load segments GDF, merge to feature DF and get class from GCPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdfs = {x:gpd.read_file(os.path.join(segmentation_path, x)) \n",
    "        for x in os.listdir(segmentation_path)\n",
    "        if x.endswith('.gpkg')}\n",
    "\n",
    "for k, df in gdfs.items():\n",
    "    fid = k.split('_')[1]\n",
    "    df['segment_id'] = df['segment_id'].apply(lambda x: f'{fid}_{x}')\n",
    "\n",
    "gdf = pd.concat(gdfs, ignore_index=True)\n",
    "\n",
    "gdf['area'] = gdf['geometry'].area\n",
    "gdf = gdf.loc[gdf.groupby('segment_id')['area'].idxmax(), :]\n",
    "\n",
    "gdf = pd.merge(gdf, fdf, left_on='segment_id', right_on='id')\n",
    "\n",
    "gdf = gpd.sjoin(gdf, gcps, how='left', op='contains')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tin           346\n",
       "concrete      290\n",
       "pavement      198\n",
       "vegetation    145\n",
       "shadow        142\n",
       "Name: class_right, dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gdf['class_right'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify and hope for the best!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop synthetic bands\n",
    "# gdf.drop(axis=1, columns=[c for c in gdf.columns if c.endswith(('_7', '_8', '_9'))], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = ['segment_id', 'area_x', 'class_left', 'geometry', 'id', 'index_right', 'idx']\n",
    "\n",
    "df = gdf[gdf['class_right'].notnull()].copy()\n",
    "df.drop(axis=1, columns=cols_to_drop, inplace=True)\n",
    "\n",
    "y = df['class_right']\n",
    "x = df.drop(['class_right'], axis=1).astype(np.float64)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=123)\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(x_train)\n",
    "\n",
    "x_train_scaled = scaler.transform(x_train)\n",
    "x_test_scaled = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test with SVM (not great)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    concrete       0.69      0.47      0.56        51\n",
      "    pavement       0.60      0.15      0.24        39\n",
      "      shadow       0.24      0.81      0.38        26\n",
      "         tin       0.56      0.43      0.49        63\n",
      "  vegetation       0.62      0.62      0.62        37\n",
      "\n",
      "   micro avg       0.47      0.47      0.47       216\n",
      "   macro avg       0.54      0.50      0.46       216\n",
      "weighted avg       0.57      0.47      0.47       216\n",
      "\n",
      "Confusion matrix:\n",
      "[[24  1  8 13  5]\n",
      " [ 0  6 31  1  1]\n",
      " [ 1  0 21  2  2]\n",
      " [10  3 17 27  6]\n",
      " [ 0  0  9  5 23]]\n",
      "Selected features:\n",
      "['compacity_index', 'mean_1', 'mean_2', 'mean_3', 'mean_5', 'median_1', 'median_2', 'median_3', 'median_4', 'median_5', 'mode_1', 'mode_2', 'mode_3', 'mode_4', 'mode_5']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "selector = SelectKBest(k = 15)\n",
    "selected_features = selector.fit_transform(x_train, y_train)\n",
    "mask = selector.get_support() #list of booleans\n",
    "new_features = [] # The list of your K best features\n",
    "\n",
    "for bool, feature in zip(mask, x_train.columns):\n",
    "    if bool:\n",
    "        new_features.append(feature)\n",
    "\n",
    "# Fit model\n",
    "x_train_fs = pd.DataFrame(selector.fit_transform(x_train, y_train),\n",
    "                      columns=new_features)\n",
    "# pipeline = make_pipeline(preprocessing.StandardScaler(),\n",
    "#                          svm.SVC())\n",
    "clf = svm.LinearSVC(max_iter=100000)\n",
    "clf.fit(x_train, y_train)\n",
    "y_pred = clf.predict(x_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print('Confusion matrix:')\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print('Selected features:')\n",
    "print(new_features)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 5 bands\n",
    "\n",
    "## Random Forest\n",
    "\n",
    "Cohen-Kappa score: 0.8292471404462779\n",
    "\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "    concrete       0.87      0.90      0.89        60\n",
    "    pavement       0.90      0.93      0.91        40\n",
    "      shadow       0.83      0.77      0.80        31\n",
    "         tin       0.88      0.86      0.87        74\n",
    "  vegetation       0.83      0.83      0.83        30\n",
    "\n",
    "   micro avg       0.87      0.87      0.87       235\n",
    "   macro avg       0.86      0.86      0.86       235\n",
    "weighted avg       0.87      0.87      0.87       235\n",
    "\n",
    "[[54  1  0  5  0]\n",
    " [ 0 37  1  1  1]\n",
    " [ 0  1 24  2  4]\n",
    " [ 8  1  1 64  0]\n",
    " [ 0  1  3  1 25]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'randomforestclassifier__max_depth': None, 'randomforestclassifier__max_features': 'sqrt', 'randomforestclassifier__min_samples_leaf': 1}\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "pipeline = make_pipeline(preprocessing.StandardScaler(),\n",
    "                         RandomForestClassifier(n_estimators=100))\n",
    "hyperparameters = {\n",
    "    'randomforestclassifier__max_depth': [None, 5, 3, 1], \n",
    "    'randomforestclassifier__max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'randomforestclassifier__min_samples_leaf': [1, 5, 20, 100]\n",
    "}\n",
    "clf = GridSearchCV(pipeline, hyperparameters, cv=10)\n",
    " \n",
    "# Fit and tune model\n",
    "clf.fit(x_train_scaled, y_train)\n",
    "\n",
    "print(clf.best_params_)\n",
    "print(clf.refit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cohen-Kappa score: 0.936736363171617\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    concrete       0.99      0.99      0.99        69\n",
      "    pavement       0.98      1.00      0.99        45\n",
      "      shadow       0.86      0.92      0.89        26\n",
      "         tin       0.93      0.96      0.95        56\n",
      "  vegetation       0.96      0.79      0.87        29\n",
      "\n",
      "   micro avg       0.95      0.95      0.95       225\n",
      "   macro avg       0.94      0.93      0.94       225\n",
      "weighted avg       0.95      0.95      0.95       225\n",
      "\n",
      "[[68  0  0  1  0]\n",
      " [ 0 45  0  0  0]\n",
      " [ 0  0 24  1  1]\n",
      " [ 1  1  0 54  0]\n",
      " [ 0  0  4  2 23]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict(x_test_scaled)\n",
    "print(f'Cohen-Kappa score: {cohen_kappa_score(y_test, y_pred)}\\n')\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/results/scalers/rgb_elev_segm.joblib']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dump(scaler, '../data/results/scalers/rgb_elev_segm.joblib')\n",
    "dump(clf, '../data/results/models/rgb_elev_segm.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll have to select columns and scale to get prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:2: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "df_reclassed = gdf.drop(axis=1, columns=cols_to_drop + ['class_right']).copy()\n",
    "scaled_array = scaler.transform(df_reclassed)\n",
    "\n",
    "df_reclassed = pd.DataFrame(scaled_array, index=df_reclassed.index, columns=df_reclassed.columns)\n",
    "df_reclassed['class_right'] = clf.predict(df_reclassed)\n",
    "\n",
    "# Join prediction to GDF, then convert to UTM and prepare fields for MBT classification\n",
    "gdf_to_save = gdf.drop(\n",
    "    axis=1,\n",
    "    columns=['class_right', 'class_left', 'area_x', 'area_y', 'id', 'index_right']\n",
    ").join(df_reclassed['class_right']).copy()\n",
    "gdf_to_save = gdf_to_save.to_crs({'init': 'epsg:32618'})\n",
    "gdf_to_save['area'] = gdf_to_save.geometry.area\n",
    "gdf_to_save['stories'] = gdf_to_save.mean_5 / 3\n",
    "gdf_to_save['utm_x'] = gdf_to_save.geometry.centroid.x\n",
    "gdf_to_save['utm_y'] = gdf_to_save.geometry.centroid.y\n",
    "\n",
    "gdf_to_save.to_file('../data/results/rgb_elev_segm.gpkg', driver='GPKG')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes\n",
    "\n",
    "Feature selection:\n",
    "\n",
    "['compacity_index', 'mean_1', 'mean_2', 'mean_3', 'mean_5', 'median_1', 'median_2', 'median_3', 'median_5', 'mode_1', 'mode_2', 'mode_3', 'mode_5', 'ASM_5', 'ASM_6']\n",
    "\n",
    "Takeaway: Our assumptions have been mostly correct. Shape is important, as well as RGB + elevation, and the texture of elevation and intensity. SWIR seems to play no role."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
